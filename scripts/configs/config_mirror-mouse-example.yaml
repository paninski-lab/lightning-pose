data:
  # dimensions of the raw training images. your computer will tell you WIDTH X HEIGHT
  image_orig_dims:
    width: 396
    height: 406
  # resize dimensions: we do this internally to accelerate training. make sure its a multiple of 128.
  image_resize_dims:
    width: 256
    height: 256
  # ABSOLUTE path to data directory. the below is just a toy example for running Getting Started scripts
  data_dir: data/mirror-mouse-example
  # ABSOLUTE path to unlabeled videos' directory. the below is just a toy example for running Getting Started scripts (TODO: Matt, can have multiple vids)
  video_dir: videos
  # location of labels; for example script, this should be relative to `data_dir`
  csv_file: CollectedData.csv
  # downsample heatmaps: 2 | 3
  downsample_factor: 2
  # total number of keypoints
  num_keypoints: 17
  # keypoint names
  keypoint_names:
    - paw1LH_top
    - paw2LF_top
    - paw3RF_top
    - paw4RH_top
    - tailBase_top
    - tailMid_top
    - nose_top
    - obs_top
    - paw1LH_bot
    - paw2LF_bot
    - paw3RF_bot
    - paw4RH_bot
    - tailBase_bot
    - tailMid_bot
    - nose_bot
    - obsHigh_bot
    - obsLow_bot
  # for mirrored setups with all keypoints defined in same csv file, define matching
  # columns for different keypoints (assumes x-y-x-y interleaving)
  # each list corresponds to a single view, so in the example below there are 2 views
  # keypoint 0 is from view 0 and matches up with keypoint 8 from view 2
  # columns that correspond to keypoints only labeled in a single view are omitted
  # this info is only used for the multiview pca loss
  mirrored_column_matches:
    - [0, 1, 2, 3, 4, 5, 6]
    - [8, 9, 10, 11, 12, 13, 14]
  columns_for_singleview_pca: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14]

training:
  # select from one of several predefined image/video augmentation pipelines
  # default: resizing only
  # dlc: imgaug pipeline implemented in DLC 2.0 package
  # dlc-light: curated subset of dlc augmentations with more conservative param settings
  imgaug: dlc
  # batch size of labeled data during training
  train_batch_size: 16
  # batch size of labeled data during validation
  val_batch_size: 16
  # batch size of labeled data during test
  # TODO: this is actually used interchangeably with sequence length in prediction. not amazing.
  test_batch_size: 16
  # fraction of labeled data used for training
  train_prob: 0.8
  # fraction of labeled data used for validation (remaining used for test)
  val_prob: 0.1
  # <=1: fraction of total train frames (determined by `train_prob`) used for training
  # >1: number of total train frames used for training
  train_frames: 1
  # number of gpus to train a single model
  num_gpus: 0
  # number of cpu workers for data loaders
  num_workers: 4
  # epochs over which to assess validation metrics for early stopping
  early_stop_patience: 3
  # epoch at which backbone network weights begin updating
  unfreezing_epoch: 20
  # min training epochs; training cannot terminate before. compare to unfreezing_epoch, check_val_every_n_epoch, early_stop_patience
  min_epochs: 300
  # max training epochs; training may exit before due to early stopping
  max_epochs: 500
  # frequency to log training metrics (one step is one batch)
  log_every_n_steps: 1
  # frequency to log validation metrics
  check_val_every_n_epoch: 10
  # select gpu for training
  gpu_id: 0
  # rng seed for labeled batches
  rng_seed_data_pt: 42
  # rng seed for weight initialization
  rng_seed_model_pt: 44
  # learning rate scheduler
  # multisteplr | [todo: reducelronplateau]
  lr_scheduler: multisteplr
  # multisteplr scheduler params
  lr_scheduler_params:
    multisteplr:
      milestones: [150, 200, 250]
      gamma: 0.5

model:
  # list of unsupervised losses:
  # - "pca_singleview"
  # - "pca_multiview"
  # - "temporal"
  # - "unimodal_mse"
  # - "unimodal_kl"
  # - null: if null, assume a model is supervised.
  losses_to_use: ["pca_singleview"]
  # backbone network:
  # resnet18 | resnet34 | resnet50 | resnet101 | resnet152 | resnet50_contrastive
  # resnet50_animalpose_apose | resnet50_animal_ap10k
  # resnet50_human_jhmdb | resnet50_human_res_rle | resnet50_human_top_res
  # efficientnet_b0 | efficientnet_b1 | efficientnet_b2
  backbone: resnet50
  # prediction mode: regression | heatmap | heatmap_mhcrnn
  model_type: heatmap
  # which heatmap loss to use: mse | kl | js
  heatmap_loss_type: mse
  # directory name for model saving
  model_name: my_base_toy_model
  # do context (5 frames)
  do_context: false

dali:
  general:
    seed: 123456
  base:
    train:
      sequence_length: 16 # step = sequence_length by default. done internally
    predict:
      sequence_length: 64 # step = sequence_length by default. done internally.
  context:
    train: # defaults: sequence_length=5, step=sequence_length
      batch_size: 6 # 6 times 5 = 30
    predict: # defaults: sequence_length=5, step=1
      sequence_length: 64

losses:
  # loss = projection onto the discarded eigenvectors
  pca_multiview:
    # weight in front of PCA loss
    log_weight: 5.0
    # predictions whould lie within the low-d subspace spanned by these components
    components_to_keep: 3
    # percentile of reprojection errors on train data below which pca loss is zeroed out
    empirical_epsilon_percentile: 1.00
    # doing eff_epsilon = percentile(error, empirical_epsilon_percentile) * empirical_epsilon_multiplier
    empirical_epsilon_multiplier: 1.0
    # absolute error (in pixels) below which pca loss is zeroed out; if not null, this
    # parameter takes precedence over `empirical_epsilon_percentile`
    epsilon: null
  # loss = projection onto the discarded eigenvectors
  pca_singleview:
    # weight in front of PCA loss
    log_weight: 5.0
    # predictions whould lie within the low-d subspace spanned by components that describe this fraction of variance
    components_to_keep: 0.99
    # percentile of reprojection errors on train data below which pca loss is zeroed out
    empirical_epsilon_percentile: 1.00
    # doing eff_epsilon = percentile(error, empirical_epsilon_percentile) * empirical_epsilon_multiplier
    empirical_epsilon_multiplier: 1.0
    # absolute error (in pixels) below which pca loss is zeroed out; if not null, this
    # parameter takes precedence over `empirical_epsilon_percentile`
    epsilon: null
  # loss = norm of distance between successive timepoints
  temporal:
    # weight in front of temporal loss
    log_weight: 5.0
    # for epsilon insensitive rectification (in pixels; diffs below this are not penalized)
    # either a float or a list of floats, one for each keypoint according to the order in the csv columns
    epsilon:
      [
        12.9,
        11.3,
        10.5,
        12.0,
        5.0,
        7.3,
        0.7,
        61.8,
        11.2,
        9.9,
        9.7,
        10.1,
        4.8,
        4.9,
        1.0,
        19.2,
        6.8,
      ]
    # nan removal value.
    # (in prob; heatmaps with max prob values are removed)
    prob_threshold: 0.05
  # loss = mse loss between generated heatmap and ideal gaussian heatmap with same center as generated heatmap
  unimodal_mse:
    # weight in front of unimodal loss
    log_weight: 6.5
    # ignore heatmaps with a max prob less than epsilon (so loss is not applied to occlusions)
    prob_threshold: 0.05
  # loss = kl loss between generated heatmap and ideal gaussian heatmap with same center as generated heatmap
  unimodal_kl:
    # weight in front of unimodal loss
    log_weight: 6.5
    # ignore heatmaps with a max prob less than epsilon (so loss is not applied to occlusions)
    prob_threshold: 0.05

  unimodal_js:
    # weight in front of unimodal loss
    log_weight: 6.5
    # ignore heatmaps with a max prob less than epsilon (so loss is not applied to occlusions)
    prob_threshold: 0.05

eval:
  # paths to the hydra config files in the output folder, OR absolute paths to such folders.
  hydra_paths: ["2021-12-15/04-11-43/"]
  # TODO: decide which arg stays and which doesn't (create_labeled_video)
  # predict?
  predict_vids_after_training: true
  # save .mp4?
  save_vids_after_training: true
  fiftyone:
    # will be the name of the dataset (Mongo DB) created by FiftyOne. for video dataset, we will append dataset_name + "_video"
    dataset_name: rick_data_test
    build_speed: slow # slow/fast. "fast" drops keypoint name and confidence information for faster processing.
    # if you want to manually provide a different model name to be displayed in FiftyOne
    model_display_names: ["test_model"]
    # whether to launch the app from the script (True), or from ipython (and have finer control over the outputs)
    launch_app_from_script: false
    remote: true # for LAI, must be False
    address: 127.0.0.1 # ip to launch the app on.
    port: 5151 # port to launch the app on.
    # whether to create a "videos" or "images" dataset, since the processes are the same
    dataset_to_create: images
  # str with an absolute path to a directory containing videos for prediction.
  # (it's not absolute just for the toy example)
  # set to null to skip automatic video prediction from train_hydra.py script
  test_videos_directory: data/mirror-mouse-example/videos
  # str with an absolute path to directory in which you want to save .csv with predictions
  saved_vid_preds_dir: data/mirror-mouse-example
  # confidence threshold for plotting a vid
  confidence_thresh_for_vid: 0.9
  # str with absolute path to the video file you want plotted with keypoints
  video_file_to_plot: /home/jovyan/lightning-pose/data/mirror-mouse-example/videos/test_vid.mp4
  # a list of strings, each points to a .csv file with predictions of a given model to the same video. will be combined with video_file_to_plot to make a visualization
  pred_csv_files_to_plot:
    ["/home/jovyan/test_vid_heatmap_pca_multiview_5.0.csv"]

callbacks:
  anneal_weight:
    attr_name: total_unsupervised_importance
    init_val: 0.0
    increase_factor: 0.01
    final_val: 1.0
    freeze_until_epoch: 0
