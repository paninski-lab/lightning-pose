{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a10ef6a",
   "metadata": {},
   "source": [
    "# ⚡ Train and visualize a Lightning Pose model ⚡\n",
    "\n",
    "Using a toy dataset (a.k.a. \"mirror-mouse\") with 90 labeled images from Warren et al., 2022 (eLife).\n",
    "* [Environment setup](#Environment-setup)\n",
    "* [Train (via PyTorch Lightning)](#Training)\n",
    "* [Monitor optimization in real time (via TensorBoard UI)](#Monitor-training)\n",
    "* [Video predictions and diagnostics](#Plot-video-predictions-and-unsupervised-losses)\n",
    "\n",
    "\n",
    "**Make sure to use a GPU runtime!**\n",
    "\n",
    "To do so, in the upper right corner of this notebook:\n",
    "* click the \"Connect\" button (or select \"Connect to a hosted runtime\" from the drop-down)\n",
    "* ensure you are connected to a GPU by clicking the down arrow, selecting \"View resources\" from the menu, and make sure you see \"Python 3 Google Compute Engine backend (GPU)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a1a5b",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the lightning-pose repository into /content/lightning-pose\n",
    "!git clone https://github.com/paninski-lab/lightning-pose.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step into that directory\n",
    "%cd /content/lightning-pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which CUDA driver is installed\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4759ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install lightning-pose as a package, including all its requirements (specified in setup.py)\n",
    "\n",
    "# NOTE: you may see the following error:\n",
    "#     ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.\n",
    "# This is fine and can be ignored\n",
    "\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d48f7a",
   "metadata": {},
   "source": [
    "#### RESTART THE RUNTIME\n",
    "Go to `Runtime > Restart session` to finish package installations.\n",
    "After restarting, proceed to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step into lightning-pose\n",
    "%cd /content/lightning-pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9356d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# read hydra configuration file from lightning-pose/scripts/configs/config_mirror-mouse-example.yaml\n",
    "# this config file contains all the necessary information for training and evaluating a\n",
    "# Lightning Pose model\n",
    "# https://lightning-pose.readthedocs.io/en/latest/source/user_guide/config_file.html\n",
    "cfg = OmegaConf.load(\"scripts/configs/config_mirror-mouse-example.yaml\")\n",
    "\n",
    "# get absolute data and video directories for toy dataset\n",
    "cfg.data.data_dir = os.path.join(\"/content/lightning-pose/data/mirror-mouse-example\")\n",
    "cfg.data.video_dir = os.path.join(\"/content/lightning-pose/data/mirror-mouse-example/videos\")\n",
    "\n",
    "assert os.path.isdir(cfg.data.data_dir), \"data_dir not a valid directory\"\n",
    "assert os.path.isdir(cfg.data.video_dir), \"video_dir not a valid directory\"\n",
    "\n",
    "# make training short for a demo (we usually do 300)\n",
    "cfg.training.min_epochs = 100\n",
    "cfg.training.max_epochs = 150\n",
    "cfg.training.batch_size = 32\n",
    "\n",
    "# directory we'll save the model in\n",
    "model_dir = '/content/outputs/semi-super-model'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b09e6",
   "metadata": {},
   "source": [
    "## Monitor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch tensorboard before launching training (happens in next cell).\n",
    "# If you receive a 403 error, be sure to enable all cookies for this site in your browser.\n",
    "# To see the losses during training, select TIME SERIES and hit the refresh button (circle arrow) on the top right.\n",
    "\n",
    "# The two most important diagnostics are:\n",
    "# - `train_supervised_rmse`: root mean square error (rmse) of predictions on training data\n",
    "# - `val_supervised_rmse`: rmse on validation data\n",
    "\n",
    "%tensorboard --logdir $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5795f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_pose.train import train\n",
    "\n",
    "# Save the model artifacts here (logs, weights, predictions)\n",
    "model_dir = '/content/outputs/semi-super-model'\n",
    "\n",
    "# Train the model (approx 15-20 mins on this T4 GPU machine)\n",
    "# This function will also:\n",
    "# - evaluate the model on train, validation, and test sets\n",
    "# - evaluate the model on a test video, and compute unsupervised losses\n",
    "model = train(cfg, model_dir=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = os.listdir(model_dir)\n",
    "print(\"Generated the following diagnostic csv files:\")\n",
    "print(artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcd9f1",
   "metadata": {},
   "source": [
    "### Display the short labeled video\n",
    "Includes network predictions.\n",
    "Make sure your video is not too large for this; it may cause memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "labeled_vid_dir = os.path.join(model_dir, \"video_preds/labeled_videos\")\n",
    "vids = os.listdir(labeled_vid_dir)\n",
    "mp4 = open(os.path.join(labeled_vid_dir, vids[0]),'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed268e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download vids to your local machine if desired\n",
    "from google.colab import files\n",
    "for vid in vids:\n",
    "    if vid.endswith(\".mp4\"):\n",
    "        files.download(os.path.join(labeled_vid_dir, vid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5e8a",
   "metadata": {},
   "source": [
    "## Plot video predictions and unsupervised losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a4b91",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from lightning_pose.apps.utils import build_precomputed_metrics_df, get_col_names, concat_dfs\n",
    "from lightning_pose.apps.utils import update_vid_metric_files_list\n",
    "from lightning_pose.apps.utils import get_model_folders, get_model_folders_vis\n",
    "from lightning_pose.apps.plots import plot_precomputed_traces\n",
    "\n",
    "# select which model(s) to use\n",
    "model_folders = get_model_folders(\"/content\")\n",
    "\n",
    "# get the last two levels of each path to be presented to user\n",
    "model_names = get_model_folders_vis(model_folders)\n",
    "\n",
    "# get prediction files for each model\n",
    "prediction_files = update_vid_metric_files_list(video=\"test_vid\", model_preds_folders=model_folders)\n",
    "\n",
    "# load data\n",
    "dframes_metrics = defaultdict(dict)\n",
    "dframes_traces = {}\n",
    "for p, model_pred_files in enumerate(prediction_files):\n",
    "    model_name = model_names[p]\n",
    "    model_folder = model_folders[p]\n",
    "    for model_pred_file in model_pred_files:\n",
    "        model_pred_file_path = os.path.join(model_folder, \"video_preds\", model_pred_file)\n",
    "        if not isinstance(model_pred_file, Path):\n",
    "            model_pred_file.seek(0)  # reset buffer after reading\n",
    "        if \"pca\" in str(model_pred_file) or \"temporal\" in str(model_pred_file) or \"pixel\" in str(model_pred_file):\n",
    "            dframe = pd.read_csv(model_pred_file_path, index_col=None)\n",
    "            dframes_metrics[model_name][str(model_pred_file)] = dframe\n",
    "        else:\n",
    "            dframe = pd.read_csv(model_pred_file_path, header=[1, 2], index_col=0)\n",
    "            dframes_traces[model_name] = dframe\n",
    "            dframes_metrics[model_name][\"confidence\"] = dframe\n",
    "        data_types = dframe.iloc[:, -1].unique()\n",
    "\n",
    "# compute metrics\n",
    "# concat dataframes, collapsing hierarchy and making df fatter.\n",
    "df_concat, keypoint_names = concat_dfs(dframes_traces)\n",
    "df_metrics = build_precomputed_metrics_df(\n",
    "    dframes=dframes_metrics, keypoint_names=keypoint_names)\n",
    "metric_options = list(df_metrics.keys())\n",
    "\n",
    "# print keypoint names; select one of these to plot below\n",
    "print(keypoint_names)\n",
    "\n",
    "# NOTE: you can ignore all errors and warnings of the type:\n",
    "#    No runtime found, using MemoryCacheStorageManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f7153",
   "metadata": {},
   "source": [
    "### Plot video traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this cell each time you want to update the keypoint\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def on_change(change):\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        clear_output()\n",
    "        cols = get_col_names(change[\"new\"], \"x\", dframes_metrics.keys())\n",
    "        fig_traces = plot_precomputed_traces(df_metrics, df_concat, cols)\n",
    "        fig_traces.show()\n",
    "\n",
    "# create a Dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=keypoint_names,\n",
    "    value=None,  # Set the default selected value\n",
    "    description=\"Select keypoint:\",\n",
    ")\n",
    "\n",
    "# update plot upon change\n",
    "dropdown.observe(on_change)\n",
    "\n",
    "# display widget\n",
    "display(dropdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
